\section{Исходный код}

Реализация состоит из двух частей: \textbf{сбор данных (Python)} и \textbf{обработка/поиск (C++)}. Все компоненты разработаны с акцентом на корректность, производительность и минимализм зависимостей.

\subsection*{Поисковый робот (\texttt{crawler.py}, \texttt{download.py})}
Робот состоит из двух модулей:
\begin{itemize}
    \item \texttt{crawler.py} — рекурсивно обходит категории и ссылки в рамках заданного раздела Wikipedia (например, \texttt{/wiki/Category:Cars}), извлекая все внутренние ссылки на статьи.
    \item \texttt{download.py} — загружает HTML-страницы по списку URL, парсит их с помощью \texttt{BeautifulSoup}, удаляет нерелевантные элементы:
    \begin{itemize}
        \item Все теги \texttt{<script>}, \texttt{<style>};
        \item Блоки с классом \texttt{navbox} (навигационные шаблоны);
        \item Таблицы с классом \texttt{infobox};
        \item Сноски и примечания.
    \end{itemize}
    Извлечённый текст сохраняется в файл \texttt{<doc\_id>.txt}.
\end{itemize}

Метаданные сохраняются в \texttt{<doc\_id>.meta.json} и включают:
\begin{itemize}
    \item Нормализованный URL;
    \item Заголовок статьи;
    \item Значения HTTP-заголовков: \texttt{Last-Modified}, \texttt{ETag};
    \item Хеш содержимого (SHA-1) для детектирования изменений.
\end{itemize}

Для обеспечения устойчивости к сбоям робот записывает состояние в \texttt{crawler\_state.json}, содержащий последний обработанный URL и счётчик скачанных документов. При повторном запуске:
\begin{enumerate}
    \item Выполняется HEAD-запрос к каждому URL;
    \item Сравниваются \texttt{Last-Modified} и \texttt{ETag} с сохранёнными;
    \item Документ перезагружается \textbf{только если изменился};
    \item Между запросами соблюдается задержка 1.5 секунды.
\end{enumerate}

Это гарантирует корректность, вежливость и экономию ресурсов.

\subsection*{Токенизатор (\texttt{tokenizer.cpp})}
Алгоритм работает в четыре этапа:
\begin{enumerate}
    \item \textbf{Чтение файла в память} в двоичном режиме (\texttt{std::ios::binary}) — чтобы избежать проблем с кодировкой (корпус в UTF-8).
    \item \textbf{Приведение к нижнему регистру} только для латинских букв A–Z (функция \texttt{to\_lower\_ascii}). Нелатинские символы (включая кириллицу, если есть) остаются без изменений.
    \item \textbf{Фильтрация}: символ считается допустимым, если он принадлежит множеству \texttt{[a–z0–9]}. Все остальные — разделители.
    \item \textbf{Формирование токенов}: при встрече разделителя накопленная последовательность проверяется на длину; токены короче 2 символов отбрасываются.
\end{enumerate}

Результат — вектор \texttt{std::string} токенов, готовых к стеммингу и индексации.

\subsection*{Стеммер (\texttt{stemmer.cpp})}
Реализован упрощённый алгоритм, вдохновлённый Портером, с итеративным применением правил:

\begin{enumerate}
    \item Слово приводится к нижнему регистру (если ещё не сделано).
    \item Выполняется цикл до стабилизации:
    \begin{itemize}
        \item Удаляются суффиксы в порядке приоритета:
        \begin{itemize}
            \item \texttt{ies} → \texttt{i}, \texttt{es} → \texttt{} (удаляется), \texttt{s} → \texttt{} (если слово длиннее 2 символов);
            \item \texttt{ed}, \texttt{ing} — при наличии гласной до последней согласной;
            \item \texttt{ly}, \texttt{ness}, \texttt{ful};
            \item \texttt{e} — если перед ним гласная и слово не оканчивается на \texttt{ee}.
        \end{itemize}
        \item После удаления \texttt{ed}/\texttt{ing} применяется правило \textit{удвоения согласной}: если слово оканчивается на CVC (согласная-гласная-согласная) и последняя согласная — не \texttt{w, x, y}, то последняя буква дублируется при добавлении суффикса. При стемминге — наоборот: одна из дублирующих согласных удаляется.
    \end{itemize}
    \item Слова короче 3 символов не стеммируются.
\end{enumerate}

Алгоритм не использует внешние библиотеки и работает за $O(k)$ для слова длины $k$.

\subsection*{Проверка закона Ципфа}
После токенизации и стемминга всего корпуса строится частотный словарь. Слова сортируются по убыванию частоты $f(r)$, где $r$ — ранг.

Закон Ципфа:  
\[
f(r) \approx \frac{C}{r}
\]

Для визуализации строится график в лог-лог масштабе:
\begin{itemize}
    \item Ось X: $\log(r)$
    \item Ось Y: $\log(f(r))$
\end{itemize}

На график наносятся:
\begin{itemize}
    \item Синие точки — эмпирические данные;
    \item Красная линия — теоретическая модель Ципфа ($C = 24$);
    \item Зелёная пунктирная линия — модель Мандельброта ($f(r) = C / (r + b)^a$, $a=1.0$, $b=0.5$).
\end{itemize}

Результаты:
\begin{itemize}
    \item Наблюдается линейная зависимость с наклоном $\approx -1$ — закон Ципфа подтверждён.
    \item Первые 10 слов составляют $\sim15\%$ всех токенов.
    \item Топ-1000 слов — $\sim50\%$ корпуса.
\end{itemize}

Это свидетельствует о естественности корпуса и пригодности для построения поисковой системы.

\subsection*{Булев индекс (\texttt{index.cpp})}
Строится два бинарных файла:

\begin{itemize}
    \item \textbf{Обратный индекс} (\texttt{inverted\_index.bin}):  
    Вектор структур \texttt{TermRecord \{ std::string term; std::vector<int> doc\_ids; \}}.  
    После построения — сортируется по \texttt{term} для бинарного поиска.
    
    \item \textbf{Прямой индекс} (\texttt{forward\_index.bin}):  
    Вектор \texttt{DocRecord \{ int doc\_id; std::string title; std::string url; \}}.  
    Порядок документов совпадает с именами файлов в \texttt{corpus\_en/}.
\end{itemize}

Сериализация — побайтовая, с явным указанием длин строк (например, сначала 4 байта — длина строки, затем сама строка). Это обеспечивает:
\begin{itemize}
    \item Компактность (без избыточного текстового формата);
    \item Быструю десериализацию (прямое чтение в структуры);
    \item Платформонезависимость при соблюдении endianness.
\end{itemize}

Сложность построения: $O(N \cdot L + M \log M)$, где $N$ — число документов, $L$ — средняя длина, $M$ — число уникальных терминов.

\subsection*{Булев поиск (\texttt{search.cpp})}
Поддерживается полный синтаксис:
\begin{itemize}
    \item \texttt{car engine} → \texttt{car \&\& engine}
    \item \texttt{car || truck}
    \item \texttt{car \&\& !bike}
    \item \texttt{(car || truck) \&\& engine}
\end{itemize}

Архитектура:
\begin{enumerate}
    \item Загрузка индексов из бинарных файлов.
    \item Лексический анализ: строка разбивается на токены — термины, операторы (\texttt{\&\&}, \texttt{||}, \texttt{!}), скобки.
    \item Синтаксический анализ методом рекурсивного спуска:
    \begin{itemize}
        \item \texttt{evaluate\_expression()} — обрабатывает OR;
        \item \texttt{evaluate\_term()} — обрабатывает AND;
        \item \texttt{evaluate\_factor()} — обрабатывает NOT и скобки.
    \end{itemize}
    \item Операции над списками \texttt{doc\_id}:
    \begin{itemize}
        \item AND — пересечение (слияние двух отсортированных списков);
        \item OR — объединение (слияние с удалением дубликатов);
        \item NOT — вычитание из полного множества документов (реализовано через \texttt{std::set} для $O(\log N)$ проверки).
    \end{itemize}
\end{enumerate}

Производительность:
\begin{itemize}
    \item Все списки \texttt{doc\_id} отсортированы → AND/OR за $O(|A| + |B|)$;
    \item Среднее время поиска — $<1$ мс на корпусе из 50 тыс. документов;
    \item Ввод/вывод: \texttt{stdin} → запрос, \texttt{stdout} → результаты, \texttt{stderr} → ошибки.
\end{itemize}

\pagebreak