\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{fontspec}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}

\setmainfont{Times New Roman}
\geometry{left=3cm, right=1.5cm, top=2cm, bottom=2cm}
\onehalfspacing
\setlist{nosep}
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

\begin{document}

\begin{center}
    {\LARGE Московский авиационный институт}\\
    {\large (национальный исследовательский университет)}\\[0.5em]
    {\large Факультет информационных технологий и прикладной математики}\\
    {\large Кафедра вычислительной математики и программирования}\\[2em]

    {\LARGE Лабораторная работа № 3}\\
    {\large по курсу «Информационный поиск»}\\[3em]

    {\large Студент: Д.\,В. Казарцев}\\
    {\large Преподаватель: А.\,А. Кухтичев}\\
    {\large Группа: М8О-410 Б}\\[4em]

    {\large Москва, 2025}
\end{center}

\newpage

\section*{Цель работы}
\begin{enumerate}
    \item Подготовить корпус документов по теме «Автомобильные статьи», который будет использоваться в последующих лабораторных работах.
    \item Ознакомиться со структурой документов: определить, из чего состоит текст, есть ли метаинформация, какая используется разметка.
    \item Выделить чистый текст, удалив навигационную обвязку, рекламу и другие нерелевантные элементы.
    \item Написать поискового робота, который автоматически собирает документы из заданных источников и сохраняет их в базе данных.
    \item Реализовать токенизацию и стемминг для подготовки текста к индексации.
    \item Проверить закон Ципфа на собранном корпусе.
    \item Реализовать булев индекс и булев поиск с поддержкой операций AND, OR, NOT и скобок.
\end{enumerate}

\section*{Описание данных}
В качестве источников данных были выбраны автомобильные статьи с двух ресурсов: английская Wikipedia и auto.ru. Эти ресурсы содержат обширные коллекции тематически релевантных текстов — от энциклопедических статей до экспертных обзоров и новостных публикаций. Wikipedia предоставляет структурированный и качественно выверенный контент через открытый доступ к HTML-страницам, а auto.ru — актуальные материалы о современных автомобилях, технологиях и тенденциях рынка.

Для сбора данных был написан поисковый робот на Python 3, который рекурсивно обходит указанные разделы, соблюдая правила вежливости (задержки между запросами, обработка robots.txt). Каждый документ сохраняется в базе данных MongoDB, где первое поле содержит нормализованный URL статьи, а второе — её очищенный текст (без HTML-разметки, навигации и рекламы). Дополнительно сохраняется метаинформация: источник, временная метка скачивания и признак обновления.

В результате был сформирован корпус из \textbf{50\,218 документов}, пригодный для последующей индексации, построения обратного индекса, токенизации, стемминга и реализации поисковой системы.

\section*{Примеры существующих поисковиков}
\begin{itemize}
    \item Рис. 1. Поиск в Google
    \item Рис. 2. Поиск в Яндекс
    \item Рис. 3. Поиск в Wiki
\end{itemize}

\section*{Исходный код}

\subsection*{Поисковый робот}
В рамках лабораторной работы №2 был разработан поисковый робот (краулер), предназначенный для автоматического сбора документов из интернета с соблюдением правил вежливости и обеспечением возможности повторной обкачки изменённых страниц.

Робот реализован на языке Python и состоит из следующих компонентов:
\begin{itemize}
    \item Сбор списка URL (\texttt{crawler.py})
    \item Основной краулер (\texttt{download.py})
\end{itemize}

\textbf{Краулер:}
\begin{itemize}
    \item Загружает HTML-страницы по списку URL.
    \item Извлекает чистый текст, удаляя:
    \begin{itemize}
        \item Навигационные блоки (\texttt{<div class="navbox">})
        \item Инфобоксы (\texttt{<table class="infobox">})
        \item Скрипты, стили и служебные элементы.
    \end{itemize}
\end{itemize}

\textbf{Сохраняет:}
\begin{itemize}
    \item Текст документа в \texttt{.txt} файл.
    \item Метаданные (URL, \texttt{Last-Modified}, \texttt{ETag}, хеш содержимого) в \texttt{.meta.json}.
\end{itemize}

\textbf{Управление состоянием:}
\begin{itemize}
    \item Робот сохраняет файл \texttt{crawler\_state.json}, содержащий:
    \begin{itemize}
        \item Последний обработанный URL.
        \item Общее количество скачанных документов.
    \end{itemize}
    \item Это позволяет остановить работу в любой момент и возобновить с того же места.
\end{itemize}

\textbf{Переобкачка изменённых документов:}
\begin{itemize}
    \item При повторном запуске робот:
    \begin{itemize}
        \item Выполняет HEAD-запрос к URL.
        \item Сравнивает заголовки \texttt{Last-Modified} и \texttt{ETag} с сохранёнными в \texttt{.meta.json}.
        \item Если документ изменился — выполняется полная перезагрузка и парсинг.
        \item Если не изменился — документ пропускается, что экономит трафик и время.
    \end{itemize}
    \item Соблюдается пауза 1.5 секунды между запросами.
\end{itemize}

После парсинга выбранных источников была сформирована папка с документами, имеющими следующие характеристики:
\begin{itemize}
    \item Количество документов — 50\,218
    \item Средний размер текстов — 7\,800 символов
\end{itemize}

\subsection*{Токенизатор (\texttt{tokenizer.cpp})}
Алгоритм токенизации работает следующим образом:
\begin{enumerate}
    \item Чтение файла в двоичном режиме (\texttt{std::ios::binary}) с помощью \texttt{std::ifstream}. Весь контент загружается в строку \texttt{std::string}.
    \item Преобразование символов: функция \texttt{to\_lower\_ascii} приводит латинские заглавные буквы (A–Z) к нижнему регистру (a–z). Остальные символы остаются без изменений.
    \item Фильтрация и разбиение: функция \texttt{is\_alphanum} допускает только латинские буквы и цифры. Остальные символы — разделители.
    \item Формирование токенов: при встрече недопустимого символа накопленная последовательность проверяется на длину. Токены короче двух символов отбрасываются.
\end{enumerate}

\subsection*{Стемминг (\texttt{stemmer.cpp})}
Алгоритм основан на эвристических правилах:
\begin{enumerate}
    \item Приведение к нижнему регистру.
    \item Последовательное удаление суффиксов в порядке приоритета:
    \begin{itemize}
        \item Формы множественного числа: \texttt{s}, \texttt{es}, \texttt{ies} → \texttt{i}
        \item Глагольные окончания: \texttt{ed}, \texttt{ing}
        \item Наречия и абстрактные существительные: \texttt{ly}, \texttt{ness}, \texttt{ful}
        \item Безударное окончание -e (если перед ним есть гласная)
    \end{itemize}
    \item Лингвистические проверки: после удаления \texttt{ing} или \texttt{ed} применяется правило удвоения согласных.
    \item Минимальная длина слова: слова короче трёх символов не подвергаются стеммингу.
\end{enumerate}

\subsection*{Закон Ципфа}
Закон Ципфа утверждает, что частота любого слова обратно пропорциональна его рангу:
\[
f(r) \approx \frac{C}{r},
\]
где \( f(r) \) — частота слова с рангом \( r \), а \( C \) — константа.

Для подтверждения закона была построена диаграмма в логарифмическом масштабе: по оси X — \( \log(\text{Rank}) \), по оси Y — \( \log(\text{Frequency}) \). Если закон выполняется, точки ложатся на прямую с наклоном ≈ -1.

\textbf{Результаты анализа:}
\begin{itemize}
    \item График показывает хорошую аппроксимацию прямой линией — закон Ципфа подтверждён.
    \item Первые 10 слов составляют ~15\% всех токенов.
    \item 1000 самых частых слов составляют ~50\% всех токенов.
\end{itemize}

\subsection*{Булев индекс (\texttt{index.cpp})}
\textbf{Структура индексов:}
\begin{itemize}
    \item \textbf{Обратный индекс} (\texttt{inverted\_index.bin}): вектор структур \texttt{TermRecord}, содержащих термин и список \texttt{doc\_ids}.
    \item \textbf{Прямой индекс} (\texttt{forward\_index.bin}): вектор структур \texttt{DocRecord} с \texttt{doc\_id}, заголовком и URL.
\end{itemize}

\textbf{Процесс индексации:}
\begin{itemize}
    \item Обход файлов в директории \texttt{corpus\_en}.
    \item Токенизация каждого файла и обновление обратного индекса.
    \item Сортировка обратного индекса по терминам.
    \item Сериализация в бинарные файлы.
\end{itemize}

\textbf{Эффективность:}
\begin{itemize}
    \item Бинарный формат — компактное хранение и быстрая загрузка.
    \item Сложность построения индекса: \( O(N \cdot L + M \cdot \log M) \), где:
    \begin{itemize}
        \item \( N \) — число документов,
        \item \( L \) — среднее число токенов,
        \item \( M \) — число уникальных терминов.
    \end{itemize}
\end{itemize}

\subsection*{Булев поиск (\texttt{search.cpp})}
\textbf{Поддерживаемый синтаксис:}
\begin{itemize}
    \item AND: \texttt{car \&\& engine} или просто \texttt{car engine}
    \item OR: \texttt{car || truck}
    \item NOT: \texttt{car \&\& !bike}
    \item Скобки: \texttt{(car || truck) \&\& engine}
\end{itemize}

\textbf{Архитектура:}
\begin{itemize}
    \item Загрузка прямого и обратного индексов.
    \item Токенизация запроса (\texttt{tokenize\_query}).
    \item Рекурсивный синтаксический анализ:
    \begin{itemize}
        \item \texttt{evaluate\_expression} — обрабатывает OR,
        \item \texttt{evaluate\_term} — обрабатывает AND,
        \item \texttt{evaluate\_factor} — обрабатывает NOT и скобки.
    \end{itemize}
    \item Операции над списками документов: AND (пересечение), OR (объединение), NOT (вычитание).
\end{itemize}

\textbf{Эффективность:}
\begin{itemize}
    \item Все списки \texttt{doc\_id} отсортированы → линейное время для AND/OR.
    \item NOT использует полный список документов и \texttt{std::set}.
    \item Среднее время выполнения запроса — менее 1 мс.
\end{itemize}

\textbf{Демонстрация поиска в веб-приложении:}
\begin{itemize}
    \item Рис. 4. Вводимый запрос
    \item Рис. 5. Вывод поиска
\end{itemize}

\section*{Вывод}
В ходе выполнения лабораторной работы была разработана полноценная поисковая система на основе корпуса из 50\,218 автомобильных статей, собранных с двух источников: английской Wikipedia и auto.ru.

Были последовательно реализованы ключевые компоненты информационно-поисковой системы:
\begin{enumerate}
    \item Поисковый робот, корректно обходящий указанные разделы и соблюдающий правила вежливости;
    \item Токенизатор и стеммер на C++;
    \item Проверка закона Ципфа, подтвердившая естественность корпуса;
    \item Прямой и обратный индексы в бинарном формате;
    \item Система булева поиска с поддержкой логических операций и скобок.
\end{enumerate}

Все компоненты системы работают автономно, масштабируемо и стабильно. Код написан с акцентом на производительность, корректность и минимальные зависимости.

\end{document}